{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b641285-843f-4660-ae7d-cf7b17afa5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from IPython.core.display import HTML "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f1a81d-13f4-46d1-a94e-0b4587013f73",
   "metadata": {},
   "source": [
    "![Title](presentation/Title.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1e7927-dbbe-4ab2-83d4-b622ac930462",
   "metadata": {},
   "source": [
    "![Cover1](presentation/Cover1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28f5dd6-9083-4302-bb0a-7b855700e1ec",
   "metadata": {},
   "source": [
    "![Slide3](presentation/Slide3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d24ebf-555d-4e72-ba63-7484e2ac295d",
   "metadata": {
    "tags": []
   },
   "source": [
    "Example: \"Mary gave roses to Susan.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b8f51a-5361-460e-9b65-2eae6ae2b53b",
   "metadata": {},
   "source": [
    "![Slide4](presentation/Slide4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14eec4a2-b7bc-452d-a31b-9645b3511c45",
   "metadata": {},
   "source": [
    "![Slide5](presentation/Slide5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a195abf0-580c-41c0-bd93-f940bd7dcb61",
   "metadata": {},
   "source": [
    "![Slide6](presentation/Slide6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ebcdbe-6530-459d-8245-83d511ba362f",
   "metadata": {},
   "source": [
    "![Slide7](presentation/Slide7.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebcc5343-4b23-4519-b169-18b3ee53bdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation for the multi-head self attention.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, emb, heads=8, mask=False):\n",
    "        \"\"\"\n",
    "        :param emb:\n",
    "        :param heads:\n",
    "        :param mask:\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        assert emb % heads == 0, f'Embedding dimension ({emb}) should be divisible by nr. of heads ({heads})'\n",
    "        \n",
    "        self.emb = emb\n",
    "        self.heads = heads\n",
    "        self.mask = mask\n",
    "        \n",
    "        s = emb // heads\n",
    "        \n",
    "        # Wk, Wq and Wv params to learn\n",
    "        self.tokeys = nn.Linear(emb, emb, bias=False)\n",
    "        self.toqueries = nn.Linear(emb, emb, bias=False)\n",
    "        self.tovalues = nn.Linear(emb, emb, bias=False)\n",
    "        \n",
    "        self.unifyheads = nn.Linear(emb, emb)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #b = batch dimension, t amount of inputs and e embed size\n",
    "        b, t, e = x.size()\n",
    "        h = self.heads\n",
    "        assert e == self.emb, f'Input embedding dim ({e}) should match layer embedding dim ({self.emb})'\n",
    "        \n",
    "        s = e // h\n",
    "        \n",
    "        keys = self.tokeys(x)\n",
    "        queries = self.toqueries(x)\n",
    "        values = self.tovalues(x)\n",
    "        \n",
    "        # we separete in different heads\n",
    "        keys = keys.view(b, t, h, s)\n",
    "        queries = queries.view(b, t, h, s)\n",
    "        values = values.view(b, t, h, s)\n",
    "        \n",
    "        # fold heads in batch dimension\n",
    "        keys = keys.transpose(1,2).contiguous().view(b * h, t, s)\n",
    "        queries = queries.transpose(1,2).contiguous().view(b * h, t, s)\n",
    "        values = values.transpose(1,2).contiguous().view(b * h, t, s)\n",
    "        \n",
    "        # compute scaled dot-product self-attention\n",
    "        queries = queries / (e ** (1/4))\n",
    "        keys = keys / (e ** (1/4))\n",
    "        \n",
    "        # dot product of queries and keys\n",
    "        dot = torch.bmm(queries, keys.transpose(1,2))\n",
    "        \n",
    "        assert dot.size() == (b * h, t, t)\n",
    "        \n",
    "        if self.mask: # this mask is for text generation that uses an autoregressive model.\n",
    "            mask_(dot, maskval=float('-inf'), mask_diagonal=False)\n",
    "            \n",
    "        dot = F.softmax(dot, dim=2)\n",
    "        # dot now has row-wise self-attention probabilities\n",
    "        \n",
    "        out = torch.bmm(dot, values).view(b, h, t, s)\n",
    "        \n",
    "        out = out.transpose(1,2).contiguous().view(b, t, s * h)\n",
    "        \n",
    "        return self.unifyheads(out)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaecec1-45f6-4817-ba05-23e92985c9ea",
   "metadata": {},
   "source": [
    "![Cover2](presentation/Cover2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fbb0ae-a208-4b86-96d0-b0450fea84be",
   "metadata": {},
   "source": [
    "![Slide8](presentation/Slide8.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25838e7b-6f01-4c29-88b4-490fb336cbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, emb, heads, mask, ff_hidden_mult=4, dropout=0.0, pos_embedding=None):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.attention = SelfAttention(emb, heads=heads, mask=mask)\n",
    "        self.mask = mask\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(emb)\n",
    "        self.norm2 = nn.LayerNorm(emb)\n",
    "        \n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(emb, ff_hidden_mult * emb),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_hidden_mult * emb, emb)\n",
    "        )\n",
    "        \n",
    "        self.do = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        attended = self.attention(x)\n",
    "        x = self.norm1(attended + x)\n",
    "        feedforward = self.ff(x)\n",
    "        x = self.norm2(feedforward + x)\n",
    "        x = self.do(x)\n",
    "        \n",
    "        return x            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5585efc2-1126-47e3-b1a8-95759efdc936",
   "metadata": {},
   "source": [
    "![Slide9](presentation/Slide9.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02f43ba8-0416-4e88-8a8a-12c477d9c6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def d(tensor=None):\n",
    "    \"\"\"\n",
    "    Returns a device string either for the best available device,\n",
    "    or for the device corresponding to the argument\n",
    "    :param tensor:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if tensor is None:\n",
    "        return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    return 'cuda' if tensor.is_cuda else 'cpu'\n",
    "\n",
    "class CTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer for sentiment analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, emb, heads, depth, seq_length, num_tokens, num_classes, max_pool=True, dropout=0.0):\n",
    "        \"\"\"\n",
    "        :param emb: Embedding dimension\n",
    "        :param heads: nr. of attention heads\n",
    "        :param depth: Number of transformer blocks\n",
    "        :param seq_length: Expected maximum sequence length\n",
    "        :param num_tokens: Number of tokens (usually words) in the vocabulary\n",
    "        :param num_classes: Number of classes.\n",
    "        :param max_pool: If true, use global max pooling in the last layer. If false, use global\n",
    "                         average pooling.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_tokens, self.max_pool = num_tokens, max_pool\n",
    "        \n",
    "        self.token_embedding = nn.Embedding(embedding_dim=emb, num_embeddings=num_tokens)\n",
    "        self.pos_embedding = nn.Embedding(embedding_dim=emb, num_embeddings=seq_length)\n",
    "        \n",
    "        tblocks = []\n",
    "        for i in range(depth):\n",
    "            tblocks.append(TransformerBlock(emb=emb, heads=heads, mask=False, dropout=dropout))\n",
    "            \n",
    "        self.tblocks = nn.Sequential(*tblocks)\n",
    "        self.toprobs = nn.Linear(emb, num_classes)\n",
    "        self.do = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: A batch by sequence length integer tensor of token indices.\n",
    "        :return: predicted log-probability vectors for each token based on the preceding tokens.\n",
    "        \"\"\"\n",
    "        \n",
    "        tokens = self.token_embedding(x)\n",
    "        b, t, e = tokens.size()\n",
    "        \n",
    "        positions = self.pos_embedding(torch.arange(t, device=d()))[None, :, :].expand(b, t, e)\n",
    "        x = tokens + positions\n",
    "        x = self.do(x)\n",
    "        x = self.tblocks(x)\n",
    "        x = x.max(dim=1)[0] if self.max_pool else x.mean(dim=1)\n",
    "        x = self.toprobs(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "11628d7c-6d4f-480f-8761-0684730afbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments.classify import batch_sampler, collate_batch\n",
    "from torchtext.datasets import IMDB\n",
    "from torchtext.vocab import vocab\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader\n",
    "from functools import partial\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "NUM_CLS = 2\n",
    "\n",
    "def go(train_step=True, batch_size=4, max_length=512, embedding_size=128,\n",
    "       num_heads=8, depth=6, vocab_size=50_000, max_pool=True,\n",
    "      lr_warmup=10_000, num_epochs=80, gradient_clipping=1.0, lr=0.0001):\n",
    "    \"\"\"\n",
    "    Creates and trains a basic Transformer for the IMDB sentiment classification task.\n",
    "    \"\"\"\n",
    "    device = 'cuda'\n",
    "    \n",
    "    if train_step:\n",
    "        train, test = IMDB(split=('train', 'test'))\n",
    "    \n",
    "    else:\n",
    "        tdata, _ = IMDB(split=('train', 'test'))\n",
    "        tdata = list(tdata)\n",
    "        random.shuffle(tdata)\n",
    "        train, test = tdata[:int(len(tdata)*0.8)], tdata[int(len(tdata)*0.8):]\n",
    "        \n",
    "    train_list = list(train)\n",
    "    test_list = list(test)\n",
    "    \n",
    "    counter = Counter()\n",
    "    \n",
    "    for (label, line) in train:\n",
    "        counter.update(tokenizer(line))\n",
    "    train_vocab = vocab(counter, min_freq=10, specials=('<unk>', '<BOS>', '<EOS>', '<PAD>'))\n",
    "    train_vocab.set_default_index(train_vocab['<unk>'])\n",
    "    \n",
    "    label_transform = lambda x: 1 if x == 'pos' else 0\n",
    "    text_transform = lambda x: [train_vocab['<BOS>']] + [train_vocab[token] for token in tokenizer(x)] + [train_vocab['<EOS>']]\n",
    "    \n",
    "    test_dataloader = DataLoader(list(test),\n",
    "                              collate_fn=partial(collate_batch, label_transform, text_transform),  \n",
    "                              batch_sampler=batch_sampler(batch_size, tokenizer, test_list))\n",
    "    \n",
    "    print(f'- nr. of training examples {len(train_list)}')\n",
    "    print(f'- nr. of {\"test\" if train_step else \"validation\"} examples {len(test_list)}')\n",
    "    \n",
    "    if max_length < 0:\n",
    "        mx = max([len(input[1]) for input in train])\n",
    "        mx = mx * 2\n",
    "        print(f'- maximum sequence length: {mx}')\n",
    "    else:\n",
    "        mx = max_length\n",
    "\n",
    "    # create the model\n",
    "    model = CTransformer(emb=embedding_size, heads=num_heads, depth=depth,\n",
    "                         seq_length=mx, num_tokens=vocab_size, num_classes=NUM_CLS, max_pool=max_pool)\n",
    "    if torch.cuda.is_available():\n",
    "        model.cuda()\n",
    "\n",
    "    opt = torch.optim.Adam(lr=lr, params=model.parameters())\n",
    "    sch = torch.optim.lr_scheduler.LambdaLR(opt, lambda i: min(i / (lr_warmup / batch_size), 1.0))\n",
    "\n",
    "    # training loop\n",
    "    seen = 0\n",
    "    for e in range(num_epochs):\n",
    "        # Dataloaders have to be created inside epoch loop so that generator starts back again from zero\n",
    "        train_dataloader = DataLoader(list(train),\n",
    "                                      collate_fn=partial(collate_batch, label_transform, text_transform),  \n",
    "                                      batch_sampler=batch_sampler(batch_size, tokenizer,train_list))\n",
    "        test_dataloader = DataLoader(list(test),\n",
    "                                      collate_fn=partial(collate_batch, label_transform, text_transform),  \n",
    "                                      batch_sampler=batch_sampler(batch_size, tokenizer,test_list))\n",
    "    \n",
    "        print(f'\\n epoch {e}')\n",
    "        print(\"Train Step\")\n",
    "        model.train(True)\n",
    "        pbar_train = tqdm(total = len(train_list) / batch_size)\n",
    "        for i, (label, input) in enumerate(train_dataloader):\n",
    "            opt.zero_grad()\n",
    "            label = label.to(device)\n",
    "            input = input.to(device)\n",
    "        \n",
    "            # print(\"label\",type(label), label.shape) \n",
    "            # print(\"input\", type(input), input.shape)\n",
    "            \n",
    "            if input.size(1) > mx:\n",
    "                input = input[:, :mx]\n",
    "            out = model(input)\n",
    "            loss = F.nll_loss(out, label)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            # clip gradients\n",
    "            # - If the total gradient vector has a length > 1, we clip it back down to 1.\n",
    "            if gradient_clipping > 0.0:\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), gradient_clipping)\n",
    "\n",
    "            opt.step()\n",
    "            sch.step()\n",
    "\n",
    "            seen += input.size(0)\n",
    "            \n",
    "            # pbar.update(batch_size)\n",
    "            # print(\"loss\", loss)\n",
    "            pbar_train.update(1)\n",
    "        pbar_train.close()\n",
    "        \n",
    "        print(\"Evaluation Step\")\n",
    "        pbar_eval = tqdm(total = len(test_list) / batch_size)\n",
    "        with torch.no_grad():\n",
    "\n",
    "            model.train(False)\n",
    "            tot, cor= 0.0, 0.0\n",
    "            for i, (label, input) in enumerate(test_dataloader):\n",
    "                label = label.to(device)\n",
    "                input = input.to(device)\n",
    "        \n",
    "        \n",
    "                if input.size(1) > mx:\n",
    "                    input = input[:, :mx]\n",
    "                out = model(input).argmax(dim=1)\n",
    "                \n",
    "                tot += float(input.size(0))\n",
    "                cor += float((label == out).sum().item())\n",
    "                pbar_eval.update(1)\n",
    "            acc = cor / tot\n",
    "            print(f'-- {\"test\" if train_step else \"validation\"} accuracy {acc:.3}')\n",
    "        pbar_eval.close()\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "54e234bf-4289-490c-baf4-e687117f03f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- nr. of training examples 25000\n",
      "- nr. of test examples 25000\n",
      "\n",
      " epoch 0\n",
      "Train Step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125.0 [01:15<00:00, 41.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125.0 [00:20<00:00, 155.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- test accuracy 0.758\n",
      "\n",
      " epoch 1\n",
      "Train Step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125.0 [01:17<00:00, 40.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125.0 [00:23<00:00, 131.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- test accuracy 0.793\n",
      "\n",
      " epoch 2\n",
      "Train Step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125.0 [01:28<00:00, 35.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125.0 [00:23<00:00, 131.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- test accuracy 0.82\n",
      "\n",
      " epoch 3\n",
      "Train Step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125.0 [01:28<00:00, 35.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125.0 [00:24<00:00, 128.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- test accuracy 0.821\n",
      "\n",
      " epoch 4\n",
      "Train Step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125.0 [01:28<00:00, 35.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125.0 [00:23<00:00, 131.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- test accuracy 0.837\n",
      "\n",
      " epoch 5\n",
      "Train Step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125.0 [01:28<00:00, 35.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125.0 [00:23<00:00, 131.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- test accuracy 0.824\n",
      "\n",
      " epoch 6\n",
      "Train Step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125.0 [01:29<00:00, 35.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125.0 [00:23<00:00, 130.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- test accuracy 0.846\n",
      "\n",
      " epoch 7\n",
      "Train Step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125.0 [01:28<00:00, 35.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125.0 [00:23<00:00, 131.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- test accuracy 0.846\n",
      "\n",
      " epoch 8\n",
      "Train Step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125.0 [01:28<00:00, 35.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125.0 [00:23<00:00, 130.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- test accuracy 0.836\n",
      "\n",
      " epoch 9\n",
      "Train Step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125.0 [01:28<00:00, 35.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3125/3125.0 [00:23<00:00, 131.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- test accuracy 0.846\n"
     ]
    }
   ],
   "source": [
    "model = go(batch_size=8, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f9e6dac0-46e9-4eef-9e3b-1cce2cbf4678",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "token_embedding.weight \t torch.Size([50000, 128])\n",
      "pos_embedding.weight \t torch.Size([512, 128])\n",
      "tblocks.0.attention.tokeys.weight \t torch.Size([128, 128])\n",
      "tblocks.0.attention.toqueries.weight \t torch.Size([128, 128])\n",
      "tblocks.0.attention.tovalues.weight \t torch.Size([128, 128])\n",
      "tblocks.0.attention.unifyheads.weight \t torch.Size([128, 128])\n",
      "tblocks.0.attention.unifyheads.bias \t torch.Size([128])\n",
      "tblocks.0.norm1.weight \t torch.Size([128])\n",
      "tblocks.0.norm1.bias \t torch.Size([128])\n",
      "tblocks.0.norm2.weight \t torch.Size([128])\n",
      "tblocks.0.norm2.bias \t torch.Size([128])\n",
      "tblocks.0.ff.0.weight \t torch.Size([512, 128])\n",
      "tblocks.0.ff.0.bias \t torch.Size([512])\n",
      "tblocks.0.ff.2.weight \t torch.Size([128, 512])\n",
      "tblocks.0.ff.2.bias \t torch.Size([128])\n",
      "tblocks.1.attention.tokeys.weight \t torch.Size([128, 128])\n",
      "tblocks.1.attention.toqueries.weight \t torch.Size([128, 128])\n",
      "tblocks.1.attention.tovalues.weight \t torch.Size([128, 128])\n",
      "tblocks.1.attention.unifyheads.weight \t torch.Size([128, 128])\n",
      "tblocks.1.attention.unifyheads.bias \t torch.Size([128])\n",
      "tblocks.1.norm1.weight \t torch.Size([128])\n",
      "tblocks.1.norm1.bias \t torch.Size([128])\n",
      "tblocks.1.norm2.weight \t torch.Size([128])\n",
      "tblocks.1.norm2.bias \t torch.Size([128])\n",
      "tblocks.1.ff.0.weight \t torch.Size([512, 128])\n",
      "tblocks.1.ff.0.bias \t torch.Size([512])\n",
      "tblocks.1.ff.2.weight \t torch.Size([128, 512])\n",
      "tblocks.1.ff.2.bias \t torch.Size([128])\n",
      "tblocks.2.attention.tokeys.weight \t torch.Size([128, 128])\n",
      "tblocks.2.attention.toqueries.weight \t torch.Size([128, 128])\n",
      "tblocks.2.attention.tovalues.weight \t torch.Size([128, 128])\n",
      "tblocks.2.attention.unifyheads.weight \t torch.Size([128, 128])\n",
      "tblocks.2.attention.unifyheads.bias \t torch.Size([128])\n",
      "tblocks.2.norm1.weight \t torch.Size([128])\n",
      "tblocks.2.norm1.bias \t torch.Size([128])\n",
      "tblocks.2.norm2.weight \t torch.Size([128])\n",
      "tblocks.2.norm2.bias \t torch.Size([128])\n",
      "tblocks.2.ff.0.weight \t torch.Size([512, 128])\n",
      "tblocks.2.ff.0.bias \t torch.Size([512])\n",
      "tblocks.2.ff.2.weight \t torch.Size([128, 512])\n",
      "tblocks.2.ff.2.bias \t torch.Size([128])\n",
      "tblocks.3.attention.tokeys.weight \t torch.Size([128, 128])\n",
      "tblocks.3.attention.toqueries.weight \t torch.Size([128, 128])\n",
      "tblocks.3.attention.tovalues.weight \t torch.Size([128, 128])\n",
      "tblocks.3.attention.unifyheads.weight \t torch.Size([128, 128])\n",
      "tblocks.3.attention.unifyheads.bias \t torch.Size([128])\n",
      "tblocks.3.norm1.weight \t torch.Size([128])\n",
      "tblocks.3.norm1.bias \t torch.Size([128])\n",
      "tblocks.3.norm2.weight \t torch.Size([128])\n",
      "tblocks.3.norm2.bias \t torch.Size([128])\n",
      "tblocks.3.ff.0.weight \t torch.Size([512, 128])\n",
      "tblocks.3.ff.0.bias \t torch.Size([512])\n",
      "tblocks.3.ff.2.weight \t torch.Size([128, 512])\n",
      "tblocks.3.ff.2.bias \t torch.Size([128])\n",
      "tblocks.4.attention.tokeys.weight \t torch.Size([128, 128])\n",
      "tblocks.4.attention.toqueries.weight \t torch.Size([128, 128])\n",
      "tblocks.4.attention.tovalues.weight \t torch.Size([128, 128])\n",
      "tblocks.4.attention.unifyheads.weight \t torch.Size([128, 128])\n",
      "tblocks.4.attention.unifyheads.bias \t torch.Size([128])\n",
      "tblocks.4.norm1.weight \t torch.Size([128])\n",
      "tblocks.4.norm1.bias \t torch.Size([128])\n",
      "tblocks.4.norm2.weight \t torch.Size([128])\n",
      "tblocks.4.norm2.bias \t torch.Size([128])\n",
      "tblocks.4.ff.0.weight \t torch.Size([512, 128])\n",
      "tblocks.4.ff.0.bias \t torch.Size([512])\n",
      "tblocks.4.ff.2.weight \t torch.Size([128, 512])\n",
      "tblocks.4.ff.2.bias \t torch.Size([128])\n",
      "tblocks.5.attention.tokeys.weight \t torch.Size([128, 128])\n",
      "tblocks.5.attention.toqueries.weight \t torch.Size([128, 128])\n",
      "tblocks.5.attention.tovalues.weight \t torch.Size([128, 128])\n",
      "tblocks.5.attention.unifyheads.weight \t torch.Size([128, 128])\n",
      "tblocks.5.attention.unifyheads.bias \t torch.Size([128])\n",
      "tblocks.5.norm1.weight \t torch.Size([128])\n",
      "tblocks.5.norm1.bias \t torch.Size([128])\n",
      "tblocks.5.norm2.weight \t torch.Size([128])\n",
      "tblocks.5.norm2.bias \t torch.Size([128])\n",
      "tblocks.5.ff.0.weight \t torch.Size([512, 128])\n",
      "tblocks.5.ff.0.bias \t torch.Size([512])\n",
      "tblocks.5.ff.2.weight \t torch.Size([128, 512])\n",
      "tblocks.5.ff.2.bias \t torch.Size([128])\n",
      "toprobs.weight \t torch.Size([2, 128])\n",
      "toprobs.bias \t torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9899372-10ed-4644-89b3-31bb30aaa0ce",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7140520d-5fd3-4945-a576-14c4727c31f5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "path_models = './models'\n",
    "model_path = os.path.join(path_models, 'presentation_model_10epochs.pth')\n",
    "torch.save(model, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca03e4b3-4a29-4c7b-9f56-ae508272df53",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2ebbee82-9a3c-4a69-a4fb-6decda20ddbd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CTransformer(\n",
       "  (token_embedding): Embedding(50000, 128)\n",
       "  (pos_embedding): Embedding(512, 128)\n",
       "  (tblocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (attention): SelfAttention(\n",
       "        (tokeys): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (toqueries): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (tovalues): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (unifyheads): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "      (do): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (attention): SelfAttention(\n",
       "        (tokeys): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (toqueries): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (tovalues): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (unifyheads): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "      (do): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (attention): SelfAttention(\n",
       "        (tokeys): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (toqueries): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (tovalues): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (unifyheads): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "      (do): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (attention): SelfAttention(\n",
       "        (tokeys): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (toqueries): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (tovalues): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (unifyheads): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "      (do): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (attention): SelfAttention(\n",
       "        (tokeys): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (toqueries): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (tovalues): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (unifyheads): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "      (do): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (attention): SelfAttention(\n",
       "        (tokeys): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (toqueries): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (tovalues): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (unifyheads): Linear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "      )\n",
       "      (do): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (toprobs): Linear(in_features=128, out_features=2, bias=True)\n",
       "  (do): Dropout(p=0.0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "path_models = './models'\n",
    "model_path = os.path.join(path_models, 'presentation_model_10epochs.pth')\n",
    "\n",
    "loaded_model = torch.load(model_path)\n",
    "loaded_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "90f8260a-5301-4b92-940c-43594c447416",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "token_embedding.weight \t torch.Size([50000, 128])\n",
      "pos_embedding.weight \t torch.Size([512, 128])\n",
      "tblocks.0.attention.tokeys.weight \t torch.Size([128, 128])\n",
      "tblocks.0.attention.toqueries.weight \t torch.Size([128, 128])\n",
      "tblocks.0.attention.tovalues.weight \t torch.Size([128, 128])\n",
      "tblocks.0.attention.unifyheads.weight \t torch.Size([128, 128])\n",
      "tblocks.0.attention.unifyheads.bias \t torch.Size([128])\n",
      "tblocks.0.norm1.weight \t torch.Size([128])\n",
      "tblocks.0.norm1.bias \t torch.Size([128])\n",
      "tblocks.0.norm2.weight \t torch.Size([128])\n",
      "tblocks.0.norm2.bias \t torch.Size([128])\n",
      "tblocks.0.ff.0.weight \t torch.Size([512, 128])\n",
      "tblocks.0.ff.0.bias \t torch.Size([512])\n",
      "tblocks.0.ff.2.weight \t torch.Size([128, 512])\n",
      "tblocks.0.ff.2.bias \t torch.Size([128])\n",
      "tblocks.1.attention.tokeys.weight \t torch.Size([128, 128])\n",
      "tblocks.1.attention.toqueries.weight \t torch.Size([128, 128])\n",
      "tblocks.1.attention.tovalues.weight \t torch.Size([128, 128])\n",
      "tblocks.1.attention.unifyheads.weight \t torch.Size([128, 128])\n",
      "tblocks.1.attention.unifyheads.bias \t torch.Size([128])\n",
      "tblocks.1.norm1.weight \t torch.Size([128])\n",
      "tblocks.1.norm1.bias \t torch.Size([128])\n",
      "tblocks.1.norm2.weight \t torch.Size([128])\n",
      "tblocks.1.norm2.bias \t torch.Size([128])\n",
      "tblocks.1.ff.0.weight \t torch.Size([512, 128])\n",
      "tblocks.1.ff.0.bias \t torch.Size([512])\n",
      "tblocks.1.ff.2.weight \t torch.Size([128, 512])\n",
      "tblocks.1.ff.2.bias \t torch.Size([128])\n",
      "tblocks.2.attention.tokeys.weight \t torch.Size([128, 128])\n",
      "tblocks.2.attention.toqueries.weight \t torch.Size([128, 128])\n",
      "tblocks.2.attention.tovalues.weight \t torch.Size([128, 128])\n",
      "tblocks.2.attention.unifyheads.weight \t torch.Size([128, 128])\n",
      "tblocks.2.attention.unifyheads.bias \t torch.Size([128])\n",
      "tblocks.2.norm1.weight \t torch.Size([128])\n",
      "tblocks.2.norm1.bias \t torch.Size([128])\n",
      "tblocks.2.norm2.weight \t torch.Size([128])\n",
      "tblocks.2.norm2.bias \t torch.Size([128])\n",
      "tblocks.2.ff.0.weight \t torch.Size([512, 128])\n",
      "tblocks.2.ff.0.bias \t torch.Size([512])\n",
      "tblocks.2.ff.2.weight \t torch.Size([128, 512])\n",
      "tblocks.2.ff.2.bias \t torch.Size([128])\n",
      "tblocks.3.attention.tokeys.weight \t torch.Size([128, 128])\n",
      "tblocks.3.attention.toqueries.weight \t torch.Size([128, 128])\n",
      "tblocks.3.attention.tovalues.weight \t torch.Size([128, 128])\n",
      "tblocks.3.attention.unifyheads.weight \t torch.Size([128, 128])\n",
      "tblocks.3.attention.unifyheads.bias \t torch.Size([128])\n",
      "tblocks.3.norm1.weight \t torch.Size([128])\n",
      "tblocks.3.norm1.bias \t torch.Size([128])\n",
      "tblocks.3.norm2.weight \t torch.Size([128])\n",
      "tblocks.3.norm2.bias \t torch.Size([128])\n",
      "tblocks.3.ff.0.weight \t torch.Size([512, 128])\n",
      "tblocks.3.ff.0.bias \t torch.Size([512])\n",
      "tblocks.3.ff.2.weight \t torch.Size([128, 512])\n",
      "tblocks.3.ff.2.bias \t torch.Size([128])\n",
      "tblocks.4.attention.tokeys.weight \t torch.Size([128, 128])\n",
      "tblocks.4.attention.toqueries.weight \t torch.Size([128, 128])\n",
      "tblocks.4.attention.tovalues.weight \t torch.Size([128, 128])\n",
      "tblocks.4.attention.unifyheads.weight \t torch.Size([128, 128])\n",
      "tblocks.4.attention.unifyheads.bias \t torch.Size([128])\n",
      "tblocks.4.norm1.weight \t torch.Size([128])\n",
      "tblocks.4.norm1.bias \t torch.Size([128])\n",
      "tblocks.4.norm2.weight \t torch.Size([128])\n",
      "tblocks.4.norm2.bias \t torch.Size([128])\n",
      "tblocks.4.ff.0.weight \t torch.Size([512, 128])\n",
      "tblocks.4.ff.0.bias \t torch.Size([512])\n",
      "tblocks.4.ff.2.weight \t torch.Size([128, 512])\n",
      "tblocks.4.ff.2.bias \t torch.Size([128])\n",
      "tblocks.5.attention.tokeys.weight \t torch.Size([128, 128])\n",
      "tblocks.5.attention.toqueries.weight \t torch.Size([128, 128])\n",
      "tblocks.5.attention.tovalues.weight \t torch.Size([128, 128])\n",
      "tblocks.5.attention.unifyheads.weight \t torch.Size([128, 128])\n",
      "tblocks.5.attention.unifyheads.bias \t torch.Size([128])\n",
      "tblocks.5.norm1.weight \t torch.Size([128])\n",
      "tblocks.5.norm1.bias \t torch.Size([128])\n",
      "tblocks.5.norm2.weight \t torch.Size([128])\n",
      "tblocks.5.norm2.bias \t torch.Size([128])\n",
      "tblocks.5.ff.0.weight \t torch.Size([512, 128])\n",
      "tblocks.5.ff.0.bias \t torch.Size([512])\n",
      "tblocks.5.ff.2.weight \t torch.Size([128, 512])\n",
      "tblocks.5.ff.2.bias \t torch.Size([128])\n",
      "toprobs.weight \t torch.Size([2, 128])\n",
      "toprobs.bias \t torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in loaded_model.state_dict():\n",
    "    print(param_tensor, \"\\t\", loaded_model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed2e2e9-5967-4739-9084-c9485134ba58",
   "metadata": {},
   "source": [
    "![Fin](https://i.imgflip.com/64xetv.gif \"fin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b75fba-1f77-4426-b495-6b1be850c6c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "former_env",
   "language": "python",
   "name": "former_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
